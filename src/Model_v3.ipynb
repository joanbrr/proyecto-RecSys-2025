{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Text Codification Pipeline\n",
        "\n",
        "This notebook implements the **Inductive Text Pipeline** for the recommender system.\n",
        "\n",
        "**Architecture:**\n",
        "1. **Item Representation:**\n",
        "   - **Review Aggregation:** An Attention mechanism summarizes up to 10 reviews into a single vector.\n",
        "   - **Gated Fusion:** A learnable gate fuses the `Overview Embedding` with the `Aggregated Review Embedding`. If reviews are missing, the model learns to rely on the overview.\n",
        "\n",
        "2. **Interaction Representation:**\n",
        "   - We treat **Ratings** as embeddings (1-5) and add them to the Item Text Embedding.\n",
        "   - $h_{interaction} = h_{item\\_text} + e_{rating}$\n",
        "\n",
        "3. **User Representation:**\n",
        "   - A sequence-based Attention mechanism (User Query) attends to the user's history of interactions to generate a dynamic user profile."
      ],
      "metadata": {
        "id": "srGsOvV3aKk-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install torch_geometric"
      ],
      "metadata": {
        "id": "ktq7AhGeg1En"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TLPMwfhGaIiQ",
        "outputId": "62f34297-59c0-4098-936a-c4be7f10a279"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from collections import defaultdict\n",
        "from torch_geometric.data import HeteroData\n",
        "from torch_geometric.nn import MessagePassing\n",
        "from torch_geometric.utils import softmax\n",
        "from recomender_metrics import evaluate_recommendations, print_evaluation_results\n",
        "\n",
        "EMBED_DIM = 1024  # Size of Alibaba-NLP/gte-large-en-v1.5 embeddings\n",
        "RATING_DIM = 1024\n",
        "HEADS = 4\n",
        "DROPOUT = 0.1\n",
        "BATCH_SIZE = 512\n",
        "LR = 1e-3\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "print(f\"Using device: {DEVICE}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Data Loading (Pre-computed Embeddings)\n",
        "\n",
        "We load the `.pt` file containing the SentenceTransformer embeddings for Overviews and Reviews.\n",
        "\n",
        "We align these tensors with our internal `movie_map` so that `movie_idx` 0 corresponds to the 0th row in these tensors."
      ],
      "metadata": {
        "id": "lWaBJNoObJ7k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_text_embeddings(pt_file_path, movie_map):\n",
        "    \"\"\"\n",
        "    Loads pre-computed text embeddings and aligns them with the movie_map.\n",
        "    \"\"\"\n",
        "    print(f\"Loading text embeddings from {pt_file_path}...\")\n",
        "\n",
        "    # Load the dictionary saved by the preprocessing step\n",
        "    data = torch.load(pt_file_path, map_location='cpu', weights_only=False)\n",
        "\n",
        "    raw_ids = data['movie_ids']       # The original CSV IDs\n",
        "    ov_embs = data['overview_embs']   # (N_samples, 384)\n",
        "    rev_embs = data['review_embs']    # (N_samples, 10, 384)\n",
        "    masks = data['review_mask']       # (N_samples, 10) - 1 if review exists, 0 if padding\n",
        "\n",
        "    num_movies = len(movie_map)\n",
        "    dim = ov_embs.shape[1]\n",
        "    max_rev = rev_embs.shape[1]\n",
        "\n",
        "    # Initialize aligned tensors\n",
        "    # ao: Aligned Overviews, ar: Aligned Reviews, am: Aligned Masks\n",
        "    ao = torch.zeros((num_movies, dim), dtype=torch.float32)\n",
        "    ar = torch.zeros((num_movies, max_rev, dim), dtype=torch.float32)\n",
        "    am = torch.zeros((num_movies, max_rev), dtype=torch.float32)\n",
        "\n",
        "    # We also need a mask to know if a movie has ANY reviews at all\n",
        "    # This helps the Gate know when to ignore the review branch entirely\n",
        "    has_reviews_mask = torch.zeros((num_movies, 1), dtype=torch.float32)\n",
        "\n",
        "    hits = 0\n",
        "    for i, mid in enumerate(raw_ids):\n",
        "        # raw_ids might be integers or strings depending on previous steps\n",
        "        # Ensure type consistency with movie_map keys\n",
        "        if mid in movie_map:\n",
        "            idx = movie_map[mid]\n",
        "            ao[idx] = ov_embs[i]\n",
        "            ar[idx] = rev_embs[i]\n",
        "            am[idx] = masks[i]\n",
        "\n",
        "            # If the sum of the mask > 0, we have at least one review\n",
        "            if masks[i].sum() > 0:\n",
        "                has_reviews_mask[idx] = 1.0\n",
        "            hits += 1\n",
        "\n",
        "    print(f\"Aligned {hits} movies out of {len(raw_ids)} raw embeddings.\")\n",
        "\n",
        "    return ao, ar, am, has_reviews_mask"
      ],
      "metadata": {
        "id": "w7IxEWqvbPQJ"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Review Aggregator & Gated Fusion\n",
        "\n",
        "This module handles the \"Static Item\" representation.\n",
        "\n",
        "1. **`ReviewAttention`**: Compresses $N$ reviews into 1 vector.\n",
        "2. **`ItemTextEncoder`**: Contains the gating logic.\n",
        "   - Formula: $h_{final} = \\lambda \\cdot h_{overview} + (1 - \\lambda) \\cdot h_{reviews}$\n",
        "   - We compute both, but if `has_reviews_mask` is 0, we force the gate to strictly use Overview."
      ],
      "metadata": {
        "id": "XdGCE_3IbWq0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ReviewAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Aggregates multiple reviews into a single embedding using Attention.\n",
        "    \"\"\"\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        # A learnable 'Query' vector that asks: \"What is the consensus of these reviews?\"\n",
        "        self.query = nn.Sequential(\n",
        "            nn.Linear(dim, 128),\n",
        "            nn.ReLu(),\n",
        "            nn.Linear(128, 1, bias=False)\n",
        "        )\n",
        "\n",
        "    def forward(self, review_embs, review_mask):\n",
        "        \"\"\"\n",
        "        review_embs: (Batch, Num_Reviews, Dim)\n",
        "        review_mask: (Batch, Num_Reviews) - 1 for valid, 0 for pad\n",
        "        \"\"\"\n",
        "        # (B, 10, 1)\n",
        "        attn_scores = self.query(review_embs)\n",
        "\n",
        "        # Masking: Set score to -infinity where mask is 0 to ignore padding\n",
        "        # We perform masked_fill on the squeezed dimension or keep dim\n",
        "        attn_scores = attn_scores.masked_fill(review_mask.unsqueeze(-1) == 0, -1e9)\n",
        "\n",
        "        # (B, 10, 1)\n",
        "        attn_weights = F.softmax(attn_scores, dim=1)\n",
        "\n",
        "        # Weighted sum: (B, 10, 1) * (B, 10, Dim) -> (B, 10, Dim) -> Sum -> (B, Dim)\n",
        "        aggregated = torch.sum(attn_weights * review_embs, dim=1)\n",
        "        return aggregated\n",
        "\n",
        "class ItemTextEncoder(nn.Module):\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        self.review_agg = ReviewAttention(dim)\n",
        "\n",
        "        # Gating mechanism\n",
        "        # Projects concatenated [Overview, Reviews] to a scalar weight [0, 1]\n",
        "        self.gate_net = nn.Sequential(\n",
        "            nn.Linear(dim * 2, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "        self.norm = nn.LayerNorm(dim)\n",
        "\n",
        "    def forward(self, ov_embs, rev_embs, rev_mask, has_rev_mask):\n",
        "        \"\"\"\n",
        "        ov_embs: (Batch, Dim)\n",
        "        rev_embs: (Batch, 10, Dim)\n",
        "        rev_mask: (Batch, 10)\n",
        "        has_rev_mask: (Batch, 1) - 1 if movie has reviews, 0 if not\n",
        "        \"\"\"\n",
        "        # Aggregate Reviews\n",
        "        # If a movie has NO reviews, this vector will be garbage (or zero),\n",
        "        # but the gate will handle it.\n",
        "        h_rev = self.review_agg(rev_embs, rev_mask)\n",
        "\n",
        "        # Compute Gate\n",
        "        # Concatenate overview and reviews to decide how much to trust each\n",
        "        combined = torch.cat([ov_embs, h_rev], dim=1)\n",
        "        alpha = self.gate_net(combined) # (Batch, 1)\n",
        "\n",
        "        # Apply Logic for Missing Reviews\n",
        "        # If has_rev_mask is 0 (no reviews), we force alpha to 1.0 (Trust Overview 100%)\n",
        "        # This overrides the learned gate for cold-start review items\n",
        "        alpha = alpha * has_rev_mask + (1.0 - has_rev_mask)\n",
        "\n",
        "        # 4. Fusion\n",
        "        h_final = alpha * ov_embs + (1 - alpha) * h_rev\n",
        "\n",
        "        return self.norm(h_final)"
      ],
      "metadata": {
        "id": "997RAGpdbaLA"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. User History Encoder\n",
        "\n",
        "This module handles the \"Dynamic User\" representation.\n",
        "\n",
        "1. **Input**: A sequence of Item Embeddings (from step 2) + Rating IDs (1-5).\n",
        "2. **Interaction Embedding**: $h_{item} + Embedding(Rating)$.\n",
        "3. **Aggregation**: A static \"User Query\" attention mechanism. This allows the model to look at the history and extract a fixed-size user vector.\n",
        "   - *Note:* We include a rating '0' for padding, so embedding size is 6."
      ],
      "metadata": {
        "id": "tJAAW3qxbiqy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class UserHistoryAttention(nn.Module):\n",
        "    def __init__(self, dim, num_rating_levels=6): # 0=Pad, 1-5=Ratings\n",
        "        super().__init__()\n",
        "\n",
        "        # Rating Embedding (Additive)\n",
        "        self.rating_emb = nn.Embedding(num_rating_levels, dim, padding_idx=0)\n",
        "\n",
        "        # The \"User Persona\" query\n",
        "        # \"Given my history, who am I?\"\n",
        "        self.user_query = nn.Parameter(torch.randn(1, dim))\n",
        "\n",
        "        # Attention Keys/Values projection\n",
        "        self.key_layer = nn.Linear(dim, dim)\n",
        "        self.value_layer = nn.Linear(dim, dim)\n",
        "\n",
        "        self.dropout = nn.Dropout(DROPOUT)\n",
        "        self.norm = nn.LayerNorm(dim)\n",
        "\n",
        "    def forward(self, item_vectors, ratings, padding_mask):\n",
        "        \"\"\"\n",
        "        item_vectors: (Batch, Seq_Len, Dim) - The text embeddings of movies watched\n",
        "        ratings: (Batch, Seq_Len) - Integers 1-5 (0 for pad)\n",
        "        padding_mask: (Batch, Seq_Len) - 1 for valid, 0 for pad\n",
        "        \"\"\"\n",
        "        # Inject Ratings\n",
        "        # Additive composition as discussed\n",
        "        r_emb = self.rating_emb(ratings)\n",
        "        interaction_vecs = item_vectors + r_emb\n",
        "\n",
        "        # Attention Mechanism\n",
        "        # Query: (1, Dim) -> Broadcast to (Batch, 1, Dim)\n",
        "        B = item_vectors.size(0)\n",
        "        Q = self.user_query.expand(B, 1, -1)\n",
        "\n",
        "        K = self.key_layer(interaction_vecs) # (B, Seq, Dim)\n",
        "        V = self.value_layer(interaction_vecs)\n",
        "\n",
        "        # Scores: Q * K^T\n",
        "        # (B, 1, D) @ (B, D, Seq) -> (B, 1, Seq)\n",
        "        scores = torch.bmm(Q, K.transpose(1, 2)) / (interaction_vecs.size(-1) ** 0.5)\n",
        "\n",
        "        # Mask out padding in history\n",
        "        scores = scores.masked_fill(padding_mask.unsqueeze(1) == 0, -1e9)\n",
        "\n",
        "        weights = F.softmax(scores, dim=-1) # (B, 1, Seq)\n",
        "        weights = self.dropout(weights)\n",
        "\n",
        "        # Weighted Sum\n",
        "        # (B, 1, Seq) @ (B, Seq, D) -> (B, 1, D)\n",
        "        user_vector = torch.bmm(weights, V).squeeze(1)\n",
        "\n",
        "        return self.norm(user_vector)\n"
      ],
      "metadata": {
        "id": "ksaIoNxdbnyQ"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. The Full Pipeline Wrapper\n",
        "\n",
        "This class orchestrates the whole flow. It holds the static embeddings (on GPU) and processes a batch of user histories."
      ],
      "metadata": {
        "id": "QOqKXGnIbt67"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TextCodificationPipeline(nn.Module):\n",
        "    def __init__(self, ov_embs, rev_embs, rev_masks, has_rev_masks, dim=1024):\n",
        "        super().__init__()\n",
        "\n",
        "        # Register the static data as buffers (not learnable parameters, but part of state)\n",
        "        # This keeps them on the correct device\n",
        "        self.register_buffer('static_ov', ov_embs)\n",
        "        self.register_buffer('static_rev', rev_embs)\n",
        "        self.register_buffer('static_rev_mask', rev_masks)\n",
        "        self.register_buffer('static_has_rev', has_rev_masks)\n",
        "\n",
        "        self.item_encoder = ItemTextEncoder(dim)\n",
        "        self.user_encoder = UserHistoryAttention(dim)\n",
        "\n",
        "    def forward(self, history_movie_ids, history_ratings, history_mask):\n",
        "        \"\"\"\n",
        "        Generates User Embeddings from their history.\n",
        "\n",
        "        history_movie_ids: (Batch, Seq_Len) - Indices in the movie_map\n",
        "        history_ratings: (Batch, Seq_Len) - Rating values (1-5)\n",
        "        history_mask: (Batch, Seq_Len) - Mask for user history length\n",
        "        \"\"\"\n",
        "\n",
        "        # Lookup Item Features for the batch\n",
        "        # We flatten the batch to process all movies at once\n",
        "        B, S = history_movie_ids.shape\n",
        "        flat_ids = history_movie_ids.view(-1) # (B*S)\n",
        "\n",
        "        batch_ov = self.static_ov[flat_ids]         # (B*S, Dim)\n",
        "        batch_rev = self.static_rev[flat_ids]       # (B*S, 10, Dim)\n",
        "        batch_rm = self.static_rev_mask[flat_ids]   # (B*S, 10)\n",
        "        batch_has_r = self.static_has_rev[flat_ids] # (B*S, 1)\n",
        "\n",
        "        # Compute Item Text Embeddings\n",
        "        # Result: (B*S, Dim)\n",
        "        flat_item_embs = self.item_encoder(batch_ov, batch_rev, batch_rm, batch_has_r)\n",
        "\n",
        "        # Reshape back to sequence\n",
        "        # (B, S, Dim)\n",
        "        seq_item_embs = flat_item_embs.view(B, S, -1)\n",
        "\n",
        "        # Compute User Embedding\n",
        "        user_emb = self.user_encoder(seq_item_embs, history_ratings, history_mask)\n",
        "\n",
        "        return user_emb"
      ],
      "metadata": {
        "id": "uZkd6urCbxRx"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "JJabmhlCfZun"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import torch\n",
        "import pandas as pd\n",
        "from torch_geometric.data import HeteroData\n",
        "import torch_geometric.transforms as T\n",
        "\n",
        "def load_data_and_build_graph_with_context(context_dir='context_data'):\n",
        "    print(\"Loading Graph Data with Context...\")\n",
        "\n",
        "    # 1. Load Standard Data\n",
        "    movies_df = pd.read_csv('movies_graph_ready.csv')\n",
        "    train_df = pd.read_csv('u1.base', sep='\\t', names=['user_id', 'movie_id', 'rating', 'timestamp'])\n",
        "    test_df = pd.read_csv('u1.test', sep='\\t', names=['user_id', 'movie_id', 'rating', 'timestamp'])\n",
        "\n",
        "    # 2. Load Context/Metadata\n",
        "    movie_ctx_df = pd.read_csv(f'{context_dir}/movie_context.csv')\n",
        "    with open(f'{context_dir}/context_metadata.json', 'r') as f:\n",
        "        ctx_meta = json.load(f)\n",
        "\n",
        "    # 3. Load Auxiliary Files (Genres, etc.) for Evaluation Mappings\n",
        "    genres_df = pd.read_csv('nodes/genres.csv')\n",
        "\n",
        "    # --- MAPPINGS ---\n",
        "    # Ensure strict alignment between movie_id and tensor index\n",
        "    movie_map = {mid: i for i, mid in enumerate(movies_df['ml_movie_id'])}\n",
        "    # Reverse map for evaluation\n",
        "    id_to_movie = {v: k for k, v in movie_map.items()}\n",
        "\n",
        "    user_map = {uid: i for i, uid in enumerate(train_df['user_id'].unique())}\n",
        "    id_to_user = {v: k for k, v in user_map.items()}\n",
        "\n",
        "    # Evaluation Mappings (Genres)\n",
        "    movie_genres_dict = {}\n",
        "    id_to_genre_name = dict(zip(genres_df['id'], genres_df['name']))\n",
        "\n",
        "    for _, row in movies_df.iterrows():\n",
        "        mid = str(row['ml_movie_id'])\n",
        "        try:\n",
        "            g_ids = json.loads(row['genres'])\n",
        "            names = {id_to_genre_name.get(gid, 'unknown') for gid in g_ids}\n",
        "            movie_genres_dict[mid] = names\n",
        "        except:\n",
        "            movie_genres_dict[mid] = set()\n",
        "\n",
        "    # Bundle all mappings\n",
        "    mappings = {\n",
        "        'user_map': user_map,\n",
        "        'movie_map': movie_map,\n",
        "        'id_to_user': id_to_user,\n",
        "        'id_to_movie': id_to_movie,\n",
        "        'movie_genres': movie_genres_dict\n",
        "    }\n",
        "\n",
        "    # --- GRAPH CONSTRUCTION ---\n",
        "    data = HeteroData()\n",
        "\n",
        "    # A. Initialize Movie Features (The Context)\n",
        "    feature_cols = ['year_bucket', 'budget_bucket', 'revenue_bucket', 'popularity_bucket',\n",
        "                   'vote_avg_bucket', 'vote_count_bucket', 'runtime_bucket']\n",
        "\n",
        "    num_movies = len(movie_map)\n",
        "    movie_feat_tensor = torch.zeros((num_movies, len(feature_cols)), dtype=torch.long)\n",
        "\n",
        "    # Fill tensor based on movie_map indices\n",
        "    for _, row in movie_ctx_df.iterrows():\n",
        "        if row['ml_movie_id'] in movie_map:\n",
        "            idx = movie_map[row['ml_movie_id']]\n",
        "            vals = row[feature_cols].values.astype(int)\n",
        "            movie_feat_tensor[idx] = torch.tensor(vals)\n",
        "\n",
        "    data['movie'].x = movie_feat_tensor\n",
        "    data['movie'].num_nodes = num_movies\n",
        "    data['user'].num_nodes = len(user_map) # Important for PyG to know user count\n",
        "\n",
        "    # B. Build Attribute Nodes & Edges\n",
        "    def load_aux_edges(filename, node_type, col_name_in_movies):\n",
        "        df = pd.read_csv(f'nodes/{filename}')\n",
        "        node_map = {id_: i for i, id_ in enumerate(df['id'])}\n",
        "        data[node_type].num_nodes = len(node_map)\n",
        "        # CRITICAL FIX: Initialize .x for attribute nodes with indices\n",
        "        data[node_type].x = torch.arange(len(node_map), dtype=torch.long)\n",
        "\n",
        "        src, dst = [], []\n",
        "        for _, row in movies_df.iterrows():\n",
        "            if row['ml_movie_id'] not in movie_map: continue\n",
        "            mid = movie_map[row['ml_movie_id']]\n",
        "            try:\n",
        "                item_ids = json.loads(row[col_name_in_movies])\n",
        "                for iid in item_ids:\n",
        "                    if iid in node_map:\n",
        "                        src.append(mid)\n",
        "                        dst.append(node_map[iid])\n",
        "            except: continue\n",
        "\n",
        "        if len(src) > 0:\n",
        "            data['movie', f'has_{node_type}', node_type].edge_index = torch.tensor([src, dst], dtype=torch.long)\n",
        "            data[node_type, f'{node_type}_of', 'movie'].edge_index = torch.tensor([dst, src], dtype=torch.long)\n",
        "\n",
        "    load_aux_edges('genres.csv', 'genre', 'genres')\n",
        "    load_aux_edges('directors.csv', 'director', 'directors')\n",
        "    load_aux_edges('keywords.csv', 'keyword', 'keywords')\n",
        "    load_aux_edges('writers.csv', 'writer', 'writers')\n",
        "\n",
        "    return data, mappings, ctx_meta, train_df, test_df"
      ],
      "metadata": {
        "id": "ZFiXZQ81fYBO"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "mKIB7mkCffxa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import HeteroConv, GATConv, Linear\n",
        "\n",
        "class ItemGraphEncoder(nn.Module):\n",
        "    def __init__(self, data_metadata, ctx_metadata, hidden_dim=64, heads=2):\n",
        "        super().__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        # Initial Embeddings (Metadata Injection)\n",
        "        # Movie Context Embeddings\n",
        "        self.movie_feat_embs = nn.ModuleList()\n",
        "        feat_keys = ['year_bucket', 'budget_bucket', 'revenue_bucket', 'popularity_bucket',\n",
        "                     'vote_avg_bucket', 'vote_count_bucket', 'runtime_bucket']\n",
        "\n",
        "        for key in feat_keys:\n",
        "            vocab_size = ctx_metadata['movie_features'][key]\n",
        "            self.movie_feat_embs.append(nn.Embedding(vocab_size, hidden_dim))\n",
        "\n",
        "        # Attribute Embeddings (Genre, Director are just indices initially)\n",
        "        # We learn a vector for every Genre, Director, etc.\n",
        "        self.attr_embs = nn.ModuleDict()\n",
        "        # The loop previously here was redundant and has been removed.\n",
        "\n",
        "        # 2. GNN Layers (The Third Relation Logic)\n",
        "\n",
        "        # Layer 1: Movies -> Attributes\n",
        "        # Attributes learn from the movies they contain\n",
        "        # \"Action Genre\" becomes a mix of \"Terminator\" + \"Matrix\" features\n",
        "        self.conv1 = HeteroConv({\n",
        "            ('movie', f'has_{nt}', nt): GATConv(hidden_dim, hidden_dim // heads, heads=heads, add_self_loops=False)\n",
        "            for nt in ['genre', 'director', 'keyword', 'writer']\n",
        "        }, aggr='sum')\n",
        "\n",
        "        # Layer 2: Attributes -> Movies\n",
        "        # Movies learn from their Enriched Attributes\n",
        "        # \"Terminator\" updates based on the \"Enriched Action Genre\"\n",
        "        self.conv2 = HeteroConv({\n",
        "            (nt, f'{nt}_of', 'movie'): GATConv(hidden_dim, hidden_dim // heads, heads=heads, add_self_loops=False)\n",
        "            for nt in ['genre', 'director', 'keyword', 'writer']\n",
        "        }, aggr='sum')\n",
        "\n",
        "        self.movie_lin = nn.Linear(hidden_dim * len(feat_keys), hidden_dim)\n",
        "        self.norm = nn.LayerNorm(hidden_dim)\n",
        "        self.relu = nn.LeakyReLU(0.2)\n",
        "\n",
        "    def init_node_embs(self, num_nodes_dict):\n",
        "        \"\"\" Initialize learnable embeddings for attribute nodes (Genre, etc.) \"\"\"\n",
        "        for ntype, count in num_nodes_dict.items():\n",
        "            if ntype != 'movie' and ntype != 'user':\n",
        "                self.attr_embs[ntype] = nn.Embedding(count, self.hidden_dim)\n",
        "\n",
        "    def forward(self, x_dict, edge_index_dict):\n",
        "        \"\"\"\n",
        "        x_dict: {'movie': [N, 7] (Metadata Indices), 'genre': [M] (Indices)...}\n",
        "        \"\"\"\n",
        "\n",
        "        # Prepare Initial Features\n",
        "        # Movies: Sum/Concat metadata embeddings\n",
        "        m_indices = x_dict['movie'] # [N, 7]\n",
        "        m_emb_list = [emb(m_indices[:, i]) for i, emb in enumerate(self.movie_feat_embs)]\n",
        "        # Stack and project: [N, 7, Dim] -> [N, Dim]\n",
        "        h_movie = self.movie_lin(torch.cat(m_emb_list, dim=1))\n",
        "\n",
        "        # Attributes: Look up learnable embeddings\n",
        "        h_dict = {'movie': h_movie}\n",
        "        for ntype, emb in self.attr_embs.items():\n",
        "            if ntype in x_dict:\n",
        "                h_dict[ntype] = emb(x_dict[ntype])\n",
        "\n",
        "        # Message Passing\n",
        "\n",
        "        # Layer 1: Propagate Movie Info TO Attributes\n",
        "        # We need to act on edges like ('movie', 'has_genre', 'genre')\n",
        "        out_l1 = self.conv1(h_dict, edge_index_dict)\n",
        "\n",
        "        # Apply activation/norm to Attribute updates\n",
        "        for ntype in out_l1:\n",
        "            # Ensure that 'movie' is not processed here if it's not a target node type in conv1\n",
        "            if ntype in h_dict and ntype != 'movie': # Added check for ntype in h_dict\n",
        "                h_dict[ntype] = self.norm(self.relu(out_l1[ntype])) # Update the h_dict for layer 2\n",
        "\n",
        "        # Layer 2: Propagate Attribute Info BACK TO Movies\n",
        "        # Input dictionary is the updated attributes + original movies\n",
        "        # (We keep original movies to drive the aggregation)\n",
        "        # h_inputs_l2 = {k: v for k, v in out_l1.items()} # Original code - needs to use updated h_dict\n",
        "\n",
        "        # Update h_dict with the output from layer 1 for next layer's input\n",
        "        # Note: out_l1 only contains destination nodes. We need to retain original sources or updated sources.\n",
        "        # The correct way is to update h_dict for the next layer's input.\n",
        "        # h_dict already contains initial movie embeddings. Just update the attribute nodes.\n",
        "\n",
        "        out_l2 = self.conv2(h_dict, edge_index_dict)\n",
        "\n",
        "        # Final Movie Embedding = Original Context + Graph Context\n",
        "        # out_l2 only contains 'movie' as its target type\n",
        "        final_movie = h_movie + out_l2['movie']\n",
        "\n",
        "        return self.norm(final_movie)\n"
      ],
      "metadata": {
        "id": "C4jQXOKDfgFw"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "5jxxdlZMfivi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class UserGraphHistoryEncoder(nn.Module):\n",
        "    def __init__(self, hidden_dim, num_time_buckets=64):\n",
        "        super().__init__()\n",
        "\n",
        "        # Interaction Embeddings\n",
        "        # Rating Embedding (1-5)\n",
        "        self.rating_emb = nn.Embedding(6, hidden_dim, padding_idx=0)\n",
        "\n",
        "        # Time Embedding (Relative Days bucketed)\n",
        "        self.time_emb = nn.Embedding(num_time_buckets, hidden_dim)\n",
        "\n",
        "        # Attention Mechanism\n",
        "        # Multi-Head Attention to capture different \"views\" of history\n",
        "        self.mha = nn.MultiheadAttention(embed_dim=hidden_dim, num_heads=HEADS, batch_first=True)\n",
        "\n",
        "        # \"User Persona\" Queries\n",
        "        # Instead of 1 query, we can have learnable queries or just self-attention\n",
        "        # Here we use a standard Self-Attention encoder block (History attends to History)\n",
        "        # followed by a Pooling/Target attention.\n",
        "\n",
        "        # To strictly follow the \"Target-Aware\" or \"User Query\" approach:\n",
        "        self.user_query = nn.Parameter(torch.randn(1, 1, hidden_dim))\n",
        "\n",
        "        self.norm = nn.LayerNorm(hidden_dim)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "\n",
        "    def forward(self, movie_graph_embs, ratings, time_buckets, mask):\n",
        "        \"\"\"\n",
        "        movie_graph_embs: [Batch, Seq, Dim] (From GNN)\n",
        "        ratings: [Batch, Seq]\n",
        "        time_buckets: [Batch, Seq]\n",
        "        \"\"\"\n",
        "\n",
        "        # Compose Interaction Vector\n",
        "        # v = h_movie + e_rating + e_time\n",
        "        r_vec = self.rating_emb(ratings)\n",
        "        t_vec = self.time_emb(time_buckets)\n",
        "\n",
        "        x = movie_graph_embs + r_vec + t_vec\n",
        "\n",
        "        # Attention\n",
        "        # User Query Attention (\"What defines me?\")\n",
        "        B, S, D = x.shape\n",
        "        query = self.user_query.expand(B, 1, D)\n",
        "\n",
        "        # Key Padding Mask: True where padding exists (PyTorch convention)\n",
        "        # Our 'mask' is usually 1 for valid, 0 for pad. PyTorch MHA expects True for Ignored positions.\n",
        "        key_padding_mask = (mask == 0)\n",
        "\n",
        "        # Attention(Q=User, K=History, V=History)\n",
        "        attn_out, _ = self.mha(query, x, x, key_padding_mask=key_padding_mask)\n",
        "\n",
        "        user_rep = attn_out.squeeze(1)\n",
        "\n",
        "        return self.norm(user_rep)"
      ],
      "metadata": {
        "id": "6cY0Anm5fjHi"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ySdbofmaflor"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class GraphCodificationPipeline(nn.Module):\n",
        "    def __init__(self, data_metadata, ctx_metadata, num_nodes_dict, hidden_dim=384):\n",
        "        super().__init__()\n",
        "\n",
        "        self.item_gnn = ItemGraphEncoder(data_metadata, ctx_metadata, hidden_dim)\n",
        "        self.item_gnn.init_node_embs(num_nodes_dict)\n",
        "\n",
        "        self.user_encoder = UserGraphHistoryEncoder(hidden_dim)\n",
        "\n",
        "    def forward(self, x_dict, edge_index_dict,\n",
        "                history_movie_indices, history_ratings, history_times, history_mask):\n",
        "        \"\"\"\n",
        "        1. Run GNN on the FULL Item Graph to get all Movie Embeddings.\n",
        "        2. Look up the specific movies in the user's history.\n",
        "        3. Encode the User History.\n",
        "        \"\"\"\n",
        "\n",
        "        # Get ALL Movie Graph Embeddings\n",
        "        # h_movies_all: [Num_Total_Movies, Dim]\n",
        "        h_movies_all = self.item_gnn(x_dict, edge_index_dict)\n",
        "\n",
        "        # Lookup History\n",
        "        # Flatten batch to index efficiently\n",
        "        B, S = history_movie_indices.shape\n",
        "        flat_ids = history_movie_indices.view(-1)\n",
        "\n",
        "        # Gather embeddings for the sequence\n",
        "        # Note: history_movie_indices must be aligned with GNN output indices\n",
        "        seq_embs = h_movies_all[flat_ids].view(B, S, -1)\n",
        "\n",
        "        # Encode User\n",
        "        user_graph_emb = self.user_encoder(seq_embs, history_ratings, history_times, history_mask)\n",
        "\n",
        "        return user_graph_emb"
      ],
      "metadata": {
        "id": "3EwPcT7sfl8s"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "B8OClro9gDjp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class DeepRecommenderSystem(nn.Module):\n",
        "    def __init__(self, text_pipeline, graph_pipeline, hidden_dim=1024, head_dim=64):\n",
        "        super().__init__()\n",
        "\n",
        "        self.text_pipeline = text_pipeline\n",
        "        self.graph_pipeline = graph_pipeline\n",
        "\n",
        "        # --- 1. SELF-ATTENTION FUSION (View merging) ---\n",
        "        # \"Which view matters more? Text or Graph?\"\n",
        "        # We learn a weight alpha to combine them: u = alpha * u_text + (1-alpha) * u_graph\n",
        "        # Implements logic from Paper\n",
        "\n",
        "        # Fusion for Users\n",
        "        self.user_fusion_gate = nn.Sequential(\n",
        "            nn.Linear(hidden_dim * 2, hidden_dim),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(hidden_dim, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "        # Fusion for Items\n",
        "        self.item_fusion_gate = nn.Sequential(\n",
        "            nn.Linear(hidden_dim * 2, hidden_dim),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(hidden_dim, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "        # --- 2. CROSS-ATTENTION (User-Item Interaction) ---\n",
        "        # \"How does this specific user feature relate to this item feature?\"\n",
        "        # Implements logic from Paper\n",
        "        self.cross_attn = nn.MultiheadAttention(embed_dim=hidden_dim, num_heads=HEADS, batch_first=True)\n",
        "        self.layer_norm = nn.LayerNorm(hidden_dim)\n",
        "\n",
        "        # --- 3. FINAL PREDICTION MLP ---\n",
        "        self.predictor = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 1) # Output Score\n",
        "        )\n",
        "\n",
        "    def get_item_embeddings(self, item_ids, item_graph_x, item_graph_edge_index):\n",
        "        \"\"\"\n",
        "        Extracts and fuses Item embeddings from both pipelines.\n",
        "        \"\"\"\n",
        "        # Text View (From pre-computed buffers in text_pipeline)\n",
        "        # item_ids needs to be flat for indexing\n",
        "        # Note: We access the internal encoders directly\n",
        "\n",
        "        # Text Encoder\n",
        "        # Retrieve raw inputs from the text pipeline's buffers\n",
        "        b_ov = self.text_pipeline.static_ov[item_ids]\n",
        "        b_rev = self.text_pipeline.static_rev[item_ids]\n",
        "        b_mask = self.text_pipeline.static_rev_mask[item_ids]\n",
        "        b_has_rev = self.text_pipeline.static_has_rev[item_ids]\n",
        "\n",
        "        i_text = self.text_pipeline.item_encoder(b_ov, b_rev, b_mask, b_has_rev)\n",
        "\n",
        "        # Graph Encoder\n",
        "        # We run the GNN to get ALL embeddings, then select the ones we need\n",
        "        # Optimization: In inference, we cache 'all_embs', but for training we re-run\n",
        "        # to get gradients through the GNN.\n",
        "        all_graph_embs = self.graph_pipeline.item_gnn(item_graph_x, item_graph_edge_index)\n",
        "        i_graph = all_graph_embs[item_ids]\n",
        "\n",
        "        # Fuse Views (Self-Attention)\n",
        "        combined = torch.cat([i_text, i_graph], dim=-1)\n",
        "        alpha = self.item_fusion_gate(combined)\n",
        "\n",
        "        i_final = alpha * i_text + (1 - alpha) * i_graph\n",
        "        return i_final, i_text, i_graph\n",
        "\n",
        "    def get_user_embeddings(self, batch_hist, batch_graph_data):\n",
        "        \"\"\"\n",
        "        Generates and fuses User embeddings from history.\n",
        "        batch_hist: Tuple (ids, rates, masks, times)\n",
        "        batch_graph_data: Tuple (x_dict, edge_index_dict)\n",
        "        \"\"\"\n",
        "        ids, rates, masks, times = batch_hist\n",
        "        x_dict, edge_idx = batch_graph_data\n",
        "\n",
        "        # Text Pipeline (User)\n",
        "        u_text = self.text_pipeline(ids, rates, masks)\n",
        "\n",
        "        # Graph Pipeline (User)\n",
        "        u_graph = self.graph_pipeline(x_dict, edge_idx, ids, rates, times, masks)\n",
        "\n",
        "        # Fuse Views (Self-Attention)\n",
        "        combined = torch.cat([u_text, u_graph], dim=-1)\n",
        "        alpha = self.user_fusion_gate(combined)\n",
        "\n",
        "        u_final = alpha * u_text + (1 - alpha) * u_graph\n",
        "\n",
        "        return u_final, u_text, u_graph\n",
        "\n",
        "    def forward(self, user_emb, item_emb):\n",
        "        \"\"\"\n",
        "        Computes the score using Cross-Attention + MLP.\n",
        "        user_emb: (Batch, Dim)\n",
        "        item_emb: (Batch, Dim)\n",
        "        \"\"\"\n",
        "        # Cross Attention requires (Batch, Seq, Dim)\n",
        "        # We treat User and Item as a sequence of length 2?\n",
        "        # Or simpler: Project User as Query, Item as Key/Value\n",
        "\n",
        "        u_q = user_emb.unsqueeze(1) # (B, 1, D)\n",
        "        i_k = item_emb.unsqueeze(1) # (B, 1, D)\n",
        "\n",
        "        # Interaction: User attends to Item\n",
        "        attn_out, _ = self.cross_attn(u_q, i_k, i_k)\n",
        "        attn_out = attn_out.squeeze(1)\n",
        "\n",
        "        interaction = attn_out * user_emb # Hadamard product to emphasize alignment\n",
        "\n",
        "        score = self.predictor(interaction)\n",
        "        return score\n",
        "\n",
        "    def get_all_embeddings(self, data, hist_ids, hist_rates, hist_times, hist_masks):\n",
        "        \"\"\"\n",
        "        Generates Fused Embeddings for ALL Users and ALL Items for inference.\n",
        "        \"\"\"\n",
        "        self.eval() # Ensure eval mode\n",
        "        with torch.no_grad():\n",
        "            all_graph_items = self.graph_pipeline.item_gnn(data.x_dict, data.edge_index_dict)\n",
        "\n",
        "            # We assume the static buffers in text_pipeline cover all items in order 0..N\n",
        "            i_ov = self.text_pipeline.static_ov\n",
        "            i_rev = self.text_pipeline.static_rev\n",
        "            i_mask = self.text_pipeline.static_rev_mask\n",
        "            i_has = self.text_pipeline.static_has_rev\n",
        "\n",
        "            i_text_all = self.text_pipeline.item_encoder(i_ov, i_rev, i_mask, i_has)\n",
        "            i_comb = torch.cat([i_text_all, all_graph_items], dim=-1)\n",
        "            i_alpha = self.item_fusion_gate(i_comb)\n",
        "            i_all_fused = i_alpha * i_text_all + (1 - i_alpha) * all_graph_items\n",
        "\n",
        "            # Text View (User History -> Text Pipe)\n",
        "            u_text_all = self.text_pipeline(hist_ids, hist_rates, hist_masks)\n",
        "\n",
        "            # Graph View (User History -> Graph Pipe)\n",
        "            # We need the specific graph embeddings for the items in history\n",
        "            B, S = hist_ids.shape\n",
        "            flat_ids = hist_ids.view(-1)\n",
        "            # Lookup from the GNN output we just computed\n",
        "            seq_graph_embs = all_graph_items[flat_ids].view(B, S, -1)\n",
        "\n",
        "            u_graph_all = self.graph_pipeline.user_encoder(\n",
        "                seq_graph_embs, hist_rates, hist_times, hist_masks\n",
        "            )\n",
        "\n",
        "            # Fuse Users\n",
        "            u_comb = torch.cat([u_text_all, u_graph_all], dim=-1)\n",
        "            u_alpha = self.user_fusion_gate(u_comb)\n",
        "            u_all_fused = u_alpha * u_text_all + (1 - u_alpha) * u_graph_all\n",
        "\n",
        "            return u_all_fused, i_all_fused"
      ],
      "metadata": {
        "id": "kWWBS1wDgD14"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "SpMD9mMkgG3G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class HybridLoss(nn.Module):\n",
        "    def __init__(self, temperature=0.1, aux_weight=0.1):\n",
        "        super().__init__()\n",
        "        self.temp = temperature\n",
        "        self.aux_weight = aux_weight # Weight for the contrastive loss\n",
        "        self.bpr_loss = nn.LogSigmoid()\n",
        "\n",
        "    def forward(self, pos_scores, neg_scores, u_text, u_graph):\n",
        "        \"\"\"\n",
        "        pos_scores: Scores for Ground Truth items\n",
        "        neg_scores: Scores for Negative sampled items\n",
        "        u_text, u_graph: The separate views of the user (before fusion)\n",
        "        \"\"\"\n",
        "\n",
        "        # Main Task: BPR Loss (Maximize Pos - Neg)\n",
        "        # Loss = -log(sigmoid(pos - neg))\n",
        "        loss_main = -torch.mean(self.bpr_loss(pos_scores - neg_scores))\n",
        "\n",
        "        # Aux Task: Self-Supervised Contrastive Loss\n",
        "        # We want u_text and u_graph to represent the SAME user.\n",
        "        # Maximize similarity(u_text, u_graph) for the same user in the batch.\n",
        "\n",
        "        # Normalize\n",
        "        u_t_norm = F.normalize(u_text, dim=1)\n",
        "        u_g_norm = F.normalize(u_graph, dim=1)\n",
        "\n",
        "        # Cosine Similarity matrix (Batch x Batch)\n",
        "        logits = torch.matmul(u_t_norm, u_g_norm.T) / self.temp\n",
        "\n",
        "        # Labels: The diagonal (0, 1, 2...) are the positive pairs\n",
        "        batch_size = u_text.shape[0]\n",
        "        labels = torch.arange(batch_size).to(u_text.device)\n",
        "\n",
        "        # InfoNCE Loss (Cross Entropy)\n",
        "        loss_aux = F.cross_entropy(logits, labels)\n",
        "\n",
        "        # Total\n",
        "        return loss_main + (self.aux_weight * loss_aux), loss_main.item(), loss_aux.item()\n"
      ],
      "metadata": {
        "id": "Tjti30Q6gHQB"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BPRDataset(Dataset):\n",
        "    def __init__(self, train_df, user_map, movie_map):\n",
        "        self.data = []\n",
        "        valid_u = set(user_map.keys())\n",
        "        valid_m = set(movie_map.keys())\n",
        "        self.user_hist = defaultdict(set)\n",
        "        self.all_items = list(movie_map.values())\n",
        "\n",
        "        pos_df = train_df[train_df['rating'] >= 4]\n",
        "\n",
        "        for u, m in zip(pos_df['user_id'], pos_df['movie_id']):\n",
        "            if u in valid_u and m in valid_m:\n",
        "                uid, mid = user_map[u], movie_map[m]\n",
        "                self.data.append((uid, mid))\n",
        "                self.user_hist[uid].add(mid)\n",
        "\n",
        "    def __len__(self): return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        u, pos = self.data[idx]\n",
        "        while True:\n",
        "            neg = np.random.choice(self.all_items)\n",
        "            if neg not in self.user_hist[u]: break\n",
        "        return torch.tensor(u), torch.tensor(pos), torch.tensor(neg)"
      ],
      "metadata": {
        "id": "GDgJ-8xBgs80"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_recommendations(model, data, mappings, train_df, hist_tensors, k=50):\n",
        "    print(\"Generating Recommendations...\")\n",
        "    model.eval()\n",
        "\n",
        "    # Unpack tensors\n",
        "    hist_ids, hist_rates, hist_times, hist_masks = hist_tensors\n",
        "\n",
        "    # Compute ALL embeddings\n",
        "    # u_all is ordered by Tensor Index (0, 1, 2... N)\n",
        "    u_all, i_all = model.get_all_embeddings(data, hist_ids, hist_rates, hist_times, hist_masks)\n",
        "\n",
        "    # Build User History for filtering\n",
        "    # Ensure keys are Integers for consistent lookup\n",
        "    user_history = train_df.groupby('user_id')['movie_id'].apply(set).to_dict()\n",
        "\n",
        "    # We will iterate through users by their TENSOR INDEX (0..N)\n",
        "    # This ensures u_all[0] matches the Real User ID at mappings['id_to_user'][0]\n",
        "    num_users = len(mappings['user_map'])\n",
        "    tensor_indices = list(range(num_users))\n",
        "\n",
        "    id_to_movie = mappings['id_to_movie']\n",
        "    id_to_user = mappings['id_to_user']\n",
        "\n",
        "    recommendations = {}\n",
        "    BATCH = 100\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i in tqdm(range(0, num_users, BATCH)):\n",
        "            # Batch of User Tensor Indices\n",
        "            batch_indices = tensor_indices[i : i + BATCH]\n",
        "            batch_u_emb = u_all[batch_indices] # (Batch, Dim)\n",
        "\n",
        "            # Score against ALL items\n",
        "            # Shape: (Batch, Num_Items)\n",
        "            scores = []\n",
        "            for u_vec in batch_u_emb:\n",
        "                # Expand User to match all items: (Num_Items, Dim)\n",
        "                u_repeated = u_vec.unsqueeze(0).expand(i_all.size(0), -1)\n",
        "\n",
        "                # Predict\n",
        "                u_scores = model(u_repeated, i_all).squeeze(-1)\n",
        "                scores.append(u_scores)\n",
        "\n",
        "            scores = torch.stack(scores)\n",
        "\n",
        "            # Rank\n",
        "            _, top_indices = torch.sort(scores, descending=True)\n",
        "            top_indices = top_indices.cpu().numpy()\n",
        "\n",
        "            # Decode to Real IDs\n",
        "            for j, u_tensor_idx in enumerate(batch_indices):\n",
        "                # Retrieve Real User ID\n",
        "                real_u_id_int = id_to_user[u_tensor_idx]\n",
        "                real_u_id_str = str(real_u_id_int)\n",
        "\n",
        "                # Retrieve Seen Set (using int key)\n",
        "                seen = user_history.get(real_u_id_int, set())\n",
        "\n",
        "                recs = []\n",
        "                for item_tensor_idx in top_indices[j]:\n",
        "                    real_item_int = id_to_movie[item_tensor_idx]\n",
        "\n",
        "                    # Filter 'seen' (Training Data)\n",
        "                    if real_item_int not in seen:\n",
        "                        recs.append(str(real_item_int))\n",
        "                        if len(recs) == k: break\n",
        "\n",
        "                recommendations[real_u_id_str] = recs\n",
        "\n",
        "    return recommendations"
      ],
      "metadata": {
        "id": "8_GZZ2VJlKBl"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_eval_data(train_df, test_df, movies_df, mappings):\n",
        "    print(\"Constructing Evaluation Dictionaries...\")\n",
        "    ground_truth = (test_df[test_df['rating'] >= 4]\n",
        "                    .groupby('user_id')\n",
        "                    .apply(lambda x: dict(zip(x['movie_id'].astype(str), x['rating'])))\n",
        "                    .to_dict())\n",
        "    ground_truth = {str(k): v for k, v in ground_truth.items()}\n",
        "\n",
        "    item_popularity = train_df.groupby('movie_id')['rating'].count().to_dict()\n",
        "    item_popularity = {str(k): v for k, v in item_popularity.items()}\n",
        "\n",
        "    item_features = mappings['movie_genres']\n",
        "    all_items = set(str(m) for m in mappings['movie_map'].keys())\n",
        "    return ground_truth, item_popularity, item_features, all_items"
      ],
      "metadata": {
        "id": "fwAUiirjmblC"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "print(\"=== 1. INITIALIZATION ===\")\n",
        "\n",
        "# Load Graph Data & Metadata\n",
        "data, mappings, ctx_meta, train_df, test_df = load_data_and_build_graph_with_context()\n",
        "data = data.to(DEVICE)\n",
        "\n",
        "# Load Text Embeddings\n",
        "ao, ar, am, has_rev = load_text_embeddings('movie_text_embeddings(2).pt', mappings['movie_map'])\n",
        "\n",
        "# Initialize Pipelines\n",
        "# Text Pipeline\n",
        "text_pipe = TextCodificationPipeline(\n",
        "    ao.to(DEVICE), ar.to(DEVICE), am.to(DEVICE), has_rev.to(DEVICE), dim=1024\n",
        ").to(DEVICE)\n",
        "\n",
        "# Graph Pipeline\n",
        "# We need the node counts for the GNN initialization\n",
        "num_nodes_dict = {nt: data[nt].num_nodes for nt in data.node_types}\n",
        "graph_pipe = GraphCodificationPipeline(\n",
        "    data.metadata(), ctx_meta, num_nodes_dict, hidden_dim=1024\n",
        ").to(DEVICE)\n",
        "\n",
        "# Deep Recommender\n",
        "model = DeepRecommenderSystem(text_pipe, graph_pipe, hidden_dim=1024).to(DEVICE)\n",
        "\n",
        "\n",
        "print(\"\\n=== 2. PRE-COMPUTING USER HISTORIES ===\")\n",
        "# We need to turn the variable-length sequences in u1.base into fixed tensors\n",
        "# that we can look up instantly during the training loop.\n",
        "\n",
        "# Load raw ratings\n",
        "ratings_df = pd.read_csv('u1.base', sep='\\t', names=['user_id', 'movie_id', 'rating', 'timestamp'])\n",
        "ratings_df['movie_id'] = ratings_df['movie_id'].map(mappings['movie_map']) # Map to internal IDs\n",
        "ratings_df = ratings_df.dropna().sort_values(['user_id', 'timestamp'])\n",
        "\n",
        "# Create Tensors: [Num_Users, Max_Seq_Len]\n",
        "num_users = len(mappings['user_map'])\n",
        "max_len = 50\n",
        "\n",
        "# Initialize with Padding (0)\n",
        "hist_ids = torch.zeros((num_users, max_len), dtype=torch.long)\n",
        "hist_rates = torch.zeros((num_users, max_len), dtype=torch.long)\n",
        "hist_times = torch.zeros((num_users, max_len), dtype=torch.long)\n",
        "hist_masks = torch.zeros((num_users, max_len), dtype=torch.float32)\n",
        "\n",
        "# Time Bucketing Helper\n",
        "# We discretize \"Time since first interaction\" into 64 buckets\n",
        "def get_time_buckets(timestamps, n_buckets=64):\n",
        "    if len(timestamps) < 2: return [0] * len(timestamps)\n",
        "    # Normalize to 0-1\n",
        "    t_min, t_max = min(timestamps), max(timestamps)\n",
        "    if t_max == t_min: return [0] * len(timestamps)\n",
        "\n",
        "    norm_time = [(t - t_min) / (t_max - t_min) for t in timestamps]\n",
        "    # Scale to 0-(n-1)\n",
        "    return [int(t * (n_buckets - 1)) for t in norm_time]\n",
        "\n",
        "print(\"Building history tensors...\")\n",
        "for uid, group in tqdm(ratings_df.groupby('user_id')):\n",
        "    if uid not in mappings['user_map']: continue\n",
        "    u_idx = mappings['user_map'][uid]\n",
        "\n",
        "    # Get sequences\n",
        "    mids = group['movie_id'].values.astype(int)[-max_len:]\n",
        "    rates = group['rating'].values.astype(int)[-max_len:]\n",
        "    times = get_time_buckets(group['timestamp'].values)[-max_len:]\n",
        "\n",
        "    seq_len = len(mids)\n",
        "\n",
        "    # Fill tensors (Left padding or Right padding? Attention usually expects padding at the end if batch_first=True)\n",
        "    # Let's fill from the beginning for simplicity with batch_first\n",
        "    hist_ids[u_idx, :seq_len] = torch.tensor(mids)\n",
        "    hist_rates[u_idx, :seq_len] = torch.tensor(rates)\n",
        "    hist_times[u_idx, :seq_len] = torch.tensor(times)\n",
        "    hist_masks[u_idx, :seq_len] = 1.0\n",
        "\n",
        "# Move to GPU for fast lookup\n",
        "hist_ids = hist_ids.to(DEVICE)\n",
        "hist_rates = hist_rates.to(DEVICE)\n",
        "hist_times = hist_times.to(DEVICE)\n",
        "hist_masks = hist_masks.to(DEVICE)\n",
        "\n",
        "print(\"\\n=== 3. TRAINING LOOP (WITH LEAKAGE FIX) ===\")\n",
        "dataset = BPRDataset(train_df, mappings['user_map'], mappings['movie_map'])\n",
        "loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
        "loss_fn = HybridLoss(aux_weight=0.1).to(DEVICE)\n",
        "\n",
        "loss_history = []\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    total_main = 0\n",
        "    total_aux = 0\n",
        "\n",
        "    pbar = tqdm(loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\")\n",
        "    for u_idx, pos_idx, neg_idx in pbar:\n",
        "        u_idx, pos_idx, neg_idx = u_idx.to(DEVICE), pos_idx.to(DEVICE), neg_idx.to(DEVICE)\n",
        "\n",
        "        # ---------------------------------------------------------\n",
        "        # STEP 1: GLOBAL GRAPH UPDATE\n",
        "        # ---------------------------------------------------------\n",
        "        all_graph_items = model.graph_pipeline.item_gnn(data.x_dict, data.edge_index_dict)\n",
        "\n",
        "        # ---------------------------------------------------------\n",
        "        # STEP 2: USER EMBEDDINGS (Two Views)\n",
        "        # ---------------------------------------------------------\n",
        "        b_h_ids = hist_ids[u_idx].clone()\n",
        "        b_h_rates = hist_rates[u_idx].clone()\n",
        "        b_h_times = hist_times[u_idx] # No clone needed if read-only\n",
        "        b_h_mask = hist_masks[u_idx].clone()\n",
        "\n",
        "        mask_prob = torch.rand_like(b_h_ids.float())\n",
        "        bert_mask = (mask_prob < 0.1) & (b_h_mask == 1)\n",
        "\n",
        "        # Hide 10% of user's liked films to treat them as targets\n",
        "        b_h_ids[bert_mask] = 0\n",
        "        b_h_mask[bert_mask] = 0\n",
        "        b_h_rates[bert_mask] = 0\n",
        "\n",
        "        # Text Pipeline (User)\n",
        "        u_text = model.text_pipeline(b_h_ids, b_h_rates, b_h_mask)\n",
        "\n",
        "        # Graph Pipeline (User)\n",
        "        B, S = b_h_ids.shape\n",
        "        flat_hist_ids = b_h_ids.view(-1)\n",
        "        seq_graph_embs = all_graph_items[flat_hist_ids].view(B, S, -1)\n",
        "\n",
        "        u_graph = model.graph_pipeline.user_encoder(\n",
        "            seq_graph_embs, b_h_rates, b_h_times, b_h_mask\n",
        "        )\n",
        "\n",
        "        # Fusion\n",
        "        u_combined = torch.cat([u_text, u_graph], dim=-1)\n",
        "        u_alpha = model.user_fusion_gate(u_combined)\n",
        "        u_final = u_alpha * u_text + (1 - u_alpha) * u_graph\n",
        "\n",
        "        # ---------------------------------------------------------\n",
        "        # STEP 3: ITEM EMBEDDINGS (Pos & Neg)\n",
        "        # ---------------------------------------------------------\n",
        "        def get_item_rep(ids):\n",
        "            i_txt_ov = model.text_pipeline.static_ov[ids]\n",
        "            i_txt_rev = model.text_pipeline.static_rev[ids]\n",
        "            i_txt_mask = model.text_pipeline.static_rev_mask[ids]\n",
        "            i_txt_has = model.text_pipeline.static_has_rev[ids]\n",
        "            i_text_emb = model.text_pipeline.item_encoder(i_txt_ov, i_txt_rev, i_txt_mask, i_txt_has)\n",
        "            i_graph_emb = all_graph_items[ids]\n",
        "            comb = torch.cat([i_text_emb, i_graph_emb], dim=-1)\n",
        "            alpha = model.item_fusion_gate(comb)\n",
        "            return alpha * i_text_emb + (1 - alpha) * i_graph_emb\n",
        "\n",
        "        pos_items_final = get_item_rep(pos_idx)\n",
        "        neg_items_final = get_item_rep(neg_idx)\n",
        "\n",
        "        # ---------------------------------------------------------\n",
        "        # STEP 4: PREDICTION & LOSS\n",
        "        # ---------------------------------------------------------\n",
        "\n",
        "        pos_scores = model(u_final, pos_items_final)\n",
        "        neg_scores = model(u_final, neg_items_final)\n",
        "\n",
        "        loss, l_main, l_aux = loss_fn(pos_scores, neg_scores, u_text, u_graph)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        total_main += l_main\n",
        "        total_aux += l_aux\n",
        "\n",
        "        pbar.set_postfix({'Loss': loss.item(), 'Main': l_main, 'Aux': l_aux})\n",
        "\n",
        "    avg_loss = total_loss / len(loader)\n",
        "    loss_history.append(avg_loss)\n",
        "    print(f\"Epoch {epoch+1} Avg Loss: {avg_loss:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UyMo7iEcgKz8",
        "outputId": "10e2875c-0cf2-4acb-90c0-6c2099230d87"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== 1. INITIALIZATION ===\n",
            "Loading Graph Data with Context...\n",
            "Loading text embeddings from movie_text_embeddings(2).pt...\n",
            "Aligned 1638 movies out of 1638 raw embeddings.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3126306.py:37: UserWarning: There exist node types ({'movie'}) whose representations do not get updated during message passing as they do not occur as destination type in any edge type. This may lead to unexpected behavior.\n",
            "  self.conv1 = HeteroConv({\n",
            "/tmp/ipython-input-3126306.py:45: UserWarning: There exist node types ({'director', 'genre'}) whose representations do not get updated during message passing as they do not occur as destination type in any edge type. This may lead to unexpected behavior.\n",
            "  self.conv2 = HeteroConv({\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== 2. PRE-COMPUTING USER HISTORIES ===\n",
            "Building history tensors...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 943/943 [00:00<00:00, 1531.64it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== 2. TRAINING LOOP (WITH LEAKAGE FIX) ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/10: 100%|| 86/86 [00:48<00:00,  1.77it/s, Loss=0.509, Main=7.21e-5]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 Avg Loss: 0.6459\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/10: 100%|| 86/86 [00:48<00:00,  1.77it/s, Loss=0.469, Main=0.000487]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 Avg Loss: 0.5885\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/10: 100%|| 86/86 [00:49<00:00,  1.75it/s, Loss=0.426, Main=0.00274]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 Avg Loss: 0.5588\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/10: 100%|| 86/86 [00:48<00:00,  1.77it/s, Loss=0.354, Main=0.00278]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4 Avg Loss: 0.4724\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/10: 100%|| 86/86 [00:48<00:00,  1.77it/s, Loss=0.318, Main=0.0151]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5 Avg Loss: 0.4272\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6/10: 100%|| 86/86 [00:48<00:00,  1.77it/s, Loss=0.273, Main=0.0131]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6 Avg Loss: 0.3843\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7/10: 100%|| 86/86 [00:48<00:00,  1.78it/s, Loss=0.273, Main=0.0022]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7 Avg Loss: 0.3713\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8/10: 100%|| 86/86 [00:48<00:00,  1.76it/s, Loss=0.256, Main=0.0188]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8 Avg Loss: 0.3491\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9/10: 100%|| 86/86 [00:48<00:00,  1.76it/s, Loss=0.234, Main=0.0114]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9 Avg Loss: 0.3261\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10/10:   6%|         | 5/86 [00:02<00:45,  1.79it/s, Loss=0.336, Main=0.0048]"
          ]
        }
      ]
    }
  ]
}